import json
import os
import datasets
import pandas as pd
from tqdm import tqdm

dataset = datasets.load_dataset(
    "json",
    data_files={
        "train": "/home/qiaoan/data/totto_data/totto_train_data.jsonl",
        # "validation": "/home/qiaoan/data/totto_data/totto_dev_data.jsonl",
    },
)
train_dataset = dataset["train"]
print("First train sample:", train_dataset[0]['table'])
# validation_dataset = dataset["validation"]
# print("Number of validation samples:", len(validation_dataset))
# print("First validation sample:", validation_dataset[0])


def to_dataframe(totto_table):
    """
    å°† TOTTO è¡¨æ ¼è½¬æ¢ä¸ºä¸¥æ ¼å¯¹é½çš„ pandas.DataFrameã€‚
    
    è§„åˆ™ï¼š
    - å±•å¼€ row_span / column_spanï¼›
    - å–æœ€åä¸€è¡Œ headerï¼›
    - åˆ é™¤è¡¨å¤´ä¸ºç©ºçš„åˆ—ï¼ˆä¸å¡«å……ï¼Œä¸å‘½åï¼‰ï¼›
    - å°†å†…å®¹ä¸º "Vacant" çš„å•å…ƒæ ¼æ›¿æ¢ä¸ºç©ºå€¼ï¼›
    - ä¸¢å¼ƒæ‰€æœ‰åˆ—æ•° != åŸå§‹ header åˆ—æ•° çš„ data è¡Œï¼›
    - è¿”å›ä¸¥æ ¼å¯¹é½çš„ DataFrameã€‚
    """
    raw_table = totto_table["table"]

    def get_true_max_cols(rows):
        return max(
            (sum(cell.get("column_span", 1) for cell in row) for row in rows),
            default=0,
        )

    def expand_table(rows, max_cols):
        grid = []
        span_map = {}  # {(src_row, col): (val, rows_left)}

        for row_idx, row in enumerate(rows):
            current_row = [None] * max_cols
            row_flags = [False] * max_cols

            # è¡¥ä¸Šè·¨è¡Œçš„å•å…ƒæ ¼
            for (r, c), (val, rspan_left) in list(span_map.items()):
                if r + 1 == row_idx and c < max_cols:
                    current_row[c] = val
                    row_flags[c] = True
                    if rspan_left > 1:
                        span_map[(row_idx, c)] = (val, rspan_left - 1)
                    del span_map[(r, c)]

            # å½“å‰è¡Œå±•å¼€
            col_ptr = 0
            for cell in row:
                val = cell["value"]
                r_span = cell.get("row_span", 1)
                c_span = cell.get("column_span", 1)

                # æ›¿æ¢ Vacant
                if isinstance(val, str) and val.strip() == "Vacant":
                    val = None

                while col_ptr < max_cols and row_flags[col_ptr]:
                    col_ptr += 1

                for offset in range(c_span):
                    idx = col_ptr + offset
                    if idx >= max_cols:
                        break
                    current_row[idx] = val
                    row_flags[idx] = True
                    if r_span > 1:
                        span_map[(row_idx, idx)] = (val, r_span - 1)
                col_ptr += c_span

            grid.append(current_row)

        return grid

    # æ‹†åˆ† header / data
    header_rows = []
    data_rows = []
    for row in raw_table:
        if all(cell.get("is_header", False) for cell in row):
            header_rows.append(row)
        else:
            data_rows.append(row)

    if not header_rows:
        return pd.DataFrame()

    max_cols = max(get_true_max_cols(header_rows), get_true_max_cols(data_rows))
    if max_cols == 0:
        return pd.DataFrame()

    header_grid = expand_table(header_rows, max_cols)
    data_grid = expand_table(data_rows, max_cols)

    last_header_row = header_grid[-1]
    orig_header_len = len(last_header_row)

    keep_idx = []
    keep_names = []
    for i, cell in enumerate(last_header_row):
        name = "" if cell is None else str(cell).strip()
        if name:
            keep_idx.append(i)
            keep_names.append(name)

    if not keep_idx:
        return pd.DataFrame()

    clean_data = []
    for row in data_grid:
        if len(row) != orig_header_len:
            continue
        filtered = [row[i] if i < len(row) else None for i in keep_idx]
        clean_data.append(filtered)

    if not clean_data:
        return pd.DataFrame(columns=keep_names)

    df = pd.DataFrame(clean_data, columns=keep_names)
    return df


output_dir = "/mnt/hdd-storage/storage_all_users/qiaoan/totto/train"
os.makedirs(output_dir, exist_ok=True)

# éå†æ•°æ®é›†ï¼Œä¿å­˜æ¯ä¸ªè¡¨æ ¼ä¸º CSV
saved_files = []
for idx, sample in enumerate(train_dataset):

    try:
        df = to_dataframe(sample)

        # æ„å»ºæ–‡ä»¶è·¯å¾„
        file_path = os.path.join(output_dir, f"{idx}.csv")

        # ä¿å­˜ä¸º UTF-8 ç¼–ç çš„ CSV
        df.to_csv(file_path, index=False, encoding="utf-8")

        saved_files.append(f"{idx}.csv")
        
        if idx % 1000 == 0:
            print(f"Saved {idx} tables...")

    except Exception as e:
        print(f"[Error] Failed on index {idx}: {e}")

# === ä¿å­˜æ–‡ä»¶åä¸º context DataFrame ===
context_df = pd.DataFrame({"context": saved_files})
context_csv_path = os.path.join(output_dir, "saved_file_list.csv")
context_df.to_csv(context_csv_path, index=False, encoding="utf-8")

print(f"\nâœ… Done. Saved {len(saved_files)} tables.")
print(f"ğŸ“„ File list written to: {context_csv_path}")